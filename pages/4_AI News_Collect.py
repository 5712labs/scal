import streamlit as st
import openai
from components import convert
import tiktoken
import feedparser
from datetime import datetime, timedelta
import deepl
from googleapiclient.discovery import build
from components.db_manager import DBManagerNews

st.set_page_config(page_title="AI NEWS", page_icon="ğŸ", layout='centered', initial_sidebar_state='auto')

if convert.check_auth() == False:
    st.stop()

st.title("AI NEWS COLLECT ğŸŒ")

db = DBManagerNews()
translator = deepl.Translator(st.secrets["api_deepl"])

clear_button = st.sidebar.button("ìƒˆ ëŒ€í™” ì‹œì‘", type="primary", key="clear_newss")
if clear_button:
    st.cache_data.clear()
    st.session_state['nmessages'] = [
        {"role": "system", "content": "You are a helpful assistant.", "hide": False}
    ]

delete_button = st.sidebar.button("ë‰´ìŠ¤ DB ì´ˆê¸°í™”", type="primary", key="del_news")
if delete_button:
    db.del_news()

get_news_list = ['í•œêµ­', 'ë¯¸êµ­', 'ìºë‚˜ë‹¤', 'ë‚˜ì´ì§€ë¦¬ì•„', 'ë² íŠ¸ë‚¨', 'ì¼ë³¸', 'ëª¨ë¡œì½”', 'UAE', 'ì¸ë„ë„¤ì‹œì•„', 'ë§ë ˆì´ì‹œì•„', 'ì§ë°”ë¸Œì›¨', 'íŒŒí‚¤ìŠ¤íƒ„', 'ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„', 'ì—í‹°ì˜¤í”¼ì•„'] #, 'ë¦¬ë¹„ì•„',  'ì´ë¼í¬', 'ìº„ë³´ë””ì•„', 'ì˜¤ë§Œ', 'íˆ¬ë¥´í¬']
get_news_range = st.multiselect("êµ­ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš” - êµ­ê°€ ë³€ê²½ ì‹œ ìˆ˜ì§‘ ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë ¤ìš”", get_news_list, ['í•œêµ­', 'ë² íŠ¸ë‚¨', 'ë‚˜ì´ì§€ë¦¬ì•„'])
gpt_feed_max = st.slider('ìµœì‹ ë‰´ìŠ¤ ê°¯ìˆ˜ë¥¼ ììœ ë¡­ê²Œ ë³€ê²½í•˜ì„¸ìš” ', 0, 50, 5)

def parse_pubdate(entry):
    published_dt = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %Z')
    formatted_str = published_dt.strftime('%Y. %m. %d')
    return formatted_str

def get_news_feed(news_range):
    sorted_all_feeds = []
    for conutry in get_news_range:
        nation = ''
        if conutry == 'í•œêµ­': # 'KR':
            rss_url = f'https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko'
            nation = 'ğŸ‡°ğŸ‡· '
            # nation = '!https://newsapi.org/images/flags/kr.svg'
        elif conutry =='ìºë‚˜ë‹¤': # 'US':
            rss_url = f'https://news.google.com/rss?hl=en-CA&gl=CA&ceid=CA:en'
            nation = 'ğŸ‡¨ğŸ‡¦ '
        elif conutry =='ë¯¸êµ­': # 'US':
            rss_url = f'https://news.google.com/rss?h|=en-US&gl=US&ceid=US:en'
            nation = 'ğŸ‡ºğŸ‡¸ '
        elif conutry == 'ì¼ë³¸': #'JP':
            rss_url = f'https://news.google.com/rss?hl=ja&gl=JP&ceid=JP:ja'
            nation = 'ğŸ‡¯ğŸ‡µ '
        elif conutry == 'ë² íŠ¸ë‚¨': #'VN':
            rss_url = f'https://news.google.com/rss?hl=vi&gl=VN&ceid=VN:vi'
            nation = 'ğŸ‡»ğŸ‡³ '
        elif conutry == 'ë‚˜ì´ì§€ë¦¬ì•„': #'NG':
            rss_url = f'https://news.google.com/rss??hl=en-NG&gl=NG&ceid=NG:en'
            nation = 'ğŸ‡³ğŸ‡¬ '
        elif conutry == 'ì§ë°”ë¸Œì›¨': #'ZW':
            rss_url = f'https://news.google.com/rss?hl=en-ZW&gl=ZW&ceid=ZW:en'
            nation = 'ğŸ‡¿ğŸ‡¼ '
        elif conutry == 'ì¸ë„ë„¤ì‹œì•„' :
            rss_url = f'https://news.google.com/rss?hl=en-ID&gl=ID&ceid=ID:en'
            nation = 'ğŸ‡®ğŸ‡© '
        elif conutry == 'íŒŒí‚¤ìŠ¤íƒ„' :
            rss_url = f'https://news.google.com/rss?hl=en-PK&gl=PK&ceid=PK:en'
            nation = 'ğŸ‡µğŸ‡° '
        elif conutry == 'ì‹±ê°€í¬ë¥´' :
            rss_url = f'https://news.google.com/rss?hl=en-SG&gl=SG&ceid=SG:en'
            nation = 'ğŸ‡¸ğŸ‡¬ '
        elif conutry == 'ì‚¬ìš°ë””ì•„ë¼ë¹„ì•„' :
            rss_url = f'https://news.google.com/rss?hl=ar&gl=SA&ceid=SA:ar'
            nation = 'ğŸ‡¸ğŸ‡¦ '
        elif conutry == 'ì—í‹°ì˜¤í”¼ì•„' :
            rss_url = f'https://news.google.com/rss?hl=en-ET&gl=ET&ceid=ET:en'
            nation = 'ğŸ‡ªğŸ‡¹ '
        elif conutry == 'ëª¨ë¡œì½”' :
            rss_url = f'https://news.google.com/rss?hl=fr&gl=MA&ceid=MA:fr'
            nation = 'ğŸ‡²ğŸ‡¦ '
        elif conutry == 'UAE' :
            rss_url = f'https://news.google.com/rss?hl=ar&gl=AE&ceid=AE:ar'
            nation = 'ğŸ‡¦ğŸ‡ª '
        elif conutry == 'ë§ë ˆì´ì‹œì•„' :
            rss_url = f'https://news.google.com/rss?hl=ms-MY&gl=MY&ceid=MY:ms'
            nation = 'ğŸ‡²ğŸ‡¾ '
        # elif conutry == 'ì´ë¼í¬' : 'ğŸ‡®ğŸ‡¶'
        # elif conutry == 'ë¦¬ë¹„ì•„' : 'ğŸ‡±ğŸ‡¾'
        # elif conutry == 'ìº„ë³´ë””ì•„' : 'ğŸ‡°ğŸ‡­'
        # elif conutry == 'ì˜¤ë§Œ' : 'ğŸ‡´ğŸ‡²'
        # elif conutry == 'íˆ¬ë¥´í¬ë©”ë‹ˆìŠ¤íƒ„' : 'ğŸ‡¹ğŸ‡²'

    # https://news.google.com/home?hl=en-ZA&gl=ZA&ceid=ZA:en ë‚¨ì•„í”„ë¦¬ì¹´
    # https://news.google.com/home?hl=ar&gl=EG&ceid=EG:ar ì´ì§‘íŠ¸
    # https://news.google.com/home?hl=te&gl=IN&ceid=IN:te ì¸ë„
    # https://news.google.com/home?hl=th&gl=TH&ceid=TH:th íƒœêµ­
        else:
            return []
        # nation = '[' + conutry + ']'
        feeds = feedparser.parse(rss_url)
        st.write(feeds)

        # ê° í•­ëª©ì˜ pubDateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤ (ìµœì‹  ë‚ ì§œê°€ ë¨¼ì € ì˜¤ë„ë¡).
        sorted_feeds = sorted(feeds.entries, key=parse_pubdate, reverse=True)
        # sorted_feeds = sorted_feeds[:gpt_feed_max]
        for idx, feed in enumerate(sorted_feeds):
            feed['num'] = idx + 1
            feed['nation'] = nation
            feed['conutry'] = conutry
        sorted_all_feeds += sorted_feeds

    return sorted_all_feeds

def get_news_feeds(get_news_range, gpt_feed_max):

    # ID nation conutry title deepl papago google link published updated_at
    # deepl = translator.translate_text(feed['title'], target_lang='KO')

    gpt_feeds = '''

    #ì…ë ¥ë¬¸
    '''

    gpt_feed_col = 3

    sorted_all_feeds = get_news_feed(get_news_range)
    # with st.expander('ìˆ˜ì§‘ ë‚´ìš© ë³´ê¸°', expanded=False):
    #     st.write(sorted_all_feeds)

    cols = st.columns(gpt_feed_col)

    updated_at = datetime.now()

    for idx, feed in enumerate(sorted_all_feeds): 
        st.write(feed)
        if gpt_feed_max < feed.num:
            continue
        result = db.get_news(feed)
        if len(result) != 0:
            continue
        
        if feed.conutry == 'í•œêµ­':
            deepl = ''
            chatgpt = ''
        else:
            deepl = translator.translate_text(feed['title'], target_lang='KO')

            prompt = f"ì•„ë˜ ë¬¸ì¥ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì¤˜ #ì•„ë˜:\n\n{feed.title}"
            messages = [{"role": "user", "content": prompt}]
            response = openai.chat.completions.create(
                model=st.session_state["openai_model"],
                messages=messages,
                # max_tokens=60,  # í•„ìš”í•œ ê²½ìš° ì–´íœ˜ ìˆ˜ë¥¼ ì¡°ì •í•˜ì‹­ì‹œì˜¤
                # temperature=0.3  # ê²°ê³¼ì˜ ì°½ì˜ì„±ì„ ì¡°ì •í•˜ëŠ” íŒŒë¼ë¯¸í„° (ë‚®ì€ ê°’ = ë” ë¦¬í„°ëŸ´í•œ ë²ˆì—­)
            )
            chatgpt = response.choices[0].message.content 
        db.save_news(feed, deepl, chatgpt, updated_at)

        pub_date = parse_pubdate(feed)
        colsnum = ( idx % gpt_feed_col )
        with cols[colsnum]:
            st.link_button(feed.nation + ' ' + feed.title + '\n\n[GPTë²ˆì—­] '+ str(chatgpt) + '\n\n[DEEPLë²ˆì—­] ' + str(deepl) + '\n\n' + f' {pub_date}', feed.link)
        gpt_feeds += f'[êµ­ê°€: {feed.conutry}] [ê¸°ì‚¬] {feed.title} \n\n'
    gpt_feeds += '''
    #ì¶œë ¥í˜•ì‹       
    [í•œêµ­] - ê¸ì • : ê¸ì • ë˜ëŠ” ë¶€ì •ê´€ë ¨ ë‹¨ì–´ 3ê°œ
    1. ê¸°ì‚¬
    2. ê¸°ì‚¬
    [ë¯¸êµ­] - ë¶€ì • : ê¸ì • ë˜ëŠ” ë¶€ì •ê´€ë ¨ ë‹¨ì–´ 3ê°œ
    1. ê¸°ì‚¬
    2. ê¸°ì‚¬
        '''
    return gpt_feeds

# date_since = (datetime.now() - timedelta(3)).strftime('%Y-%m-%d')
# stock_name = 'ê±´ì„¤'
# rss_url = f'https://news.google.com/rss/search?q={stock_name}+after:{date_since}&hl=ko&gl=KR&ceid=KR%3Ako'
# feeds = feedparser.parse(rss_url)
# st.write(feeds)

if st.button('ìˆ˜ì§‘í•˜ê¸°', type="primary"):
    gpt_feeds = get_news_feeds(get_news_range, gpt_feed_max)

st.stop()

# @st.cache_data
def get_tube_feed():
    youtube = build("youtube", "v3",developerKey=st.secrets["api_youtube"])
    search_response = youtube.search().list(
    q = 'ê±´ì„¤',
    order = "relevance",
    part = "snippet",
    maxResults = 9
    ).execute()

    def info_to_dict(videoId, title, description, url):
        result = {
            "videoId": videoId,
            "title": title,
            "description": description,
            "url": url
        }
        return result

    result_json = {}
    idx =0
    for item in search_response['items']:
        if item['id']['kind'] == 'youtube#video':
            result_json[idx] = info_to_dict(item['id']['videoId'], item['snippet']['title'], item['snippet']['description'], item['snippet']['thumbnails']['medium']['url'])
            idx += 1
    
    gpt_feed_col = 3
    cols = st.columns(gpt_feed_col)
    for idx, item in enumerate(search_response['items']):
        if item['id']['kind'] == 'youtube#video':
            colsnum = ( idx % gpt_feed_col )
            with cols[colsnum]:
                st.image(item['snippet']['thumbnails']['medium']['url'])

get_tube_feed()

if "nmessages" not in st.session_state:
    st.session_state.nmessages = []

for message in st.session_state.nmessages:
    if message["hide"] == False:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

prompt = st.chat_input("Say something")

prompt1_button = st.button("GPT ìš”ì•½í•˜ê¸°", type="primary", key="prompt1")
if prompt1_button:
    gpt_feed = '''
    #ëª…ë ¹ë¬¸
    ì•„ë˜ì˜ ì œì•½ì¡°ê±´ê³¼ ì…ë ¥ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ
    [êµ­ê°€]ë³„ë¡œ êµ¬ë¶„í•´ì„œ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”
    ìš”ì•½ í›„ì—ëŠ” 'ê¸ì •' ë˜ëŠ” 'ë¶€ì •' ë‘˜ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”
    ì¶”ê°€ ì„¤ëª…ì—†ì´ ê¸ì • ë˜ëŠ” ë¶€ì •ê³¼ ê´€ë ¨ëœ ë‹¨ì–´ 3ê°œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”
    
    #ì œì•½ì¡°ê±´
    ìš”ì ì„ ëª…í™•íˆ í•œë‹¤
    ë¬¸ì¥ì€ ê°„ê²°í•˜ê²Œ ì•Œê¸° ì‰½ê²Œ ì“´ë‹¤

    '''
    gpt_feeds = gpt_feed + gpt_feeds

    prompt = gpt_feeds
    st.session_state['nmessages'] = [
        {"role": "user", "content": gpt_feeds, 'hide': True}
    ]

prompt2_button = st.button("ê±´ì„¤ ë‰´ìŠ¤ ìš”ì•½í•˜ê¸°", type="primary", key="prompt2")
if prompt2_button:

    gpt_feed = '''
    #ëª…ë ¹ë¬¸
    ì•„ë˜ì˜ ì œì•½ì¡°ê±´ê³¼ ì…ë ¥ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ
    [êµ­ê°€]ë³„ë¡œ ê±´ì„¤ ê´€ë ¨ ê¸°ì‚¬ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”
    ìš”ì•½ í›„ì—ëŠ” 'ê¸ì •' ë˜ëŠ” 'ë¶€ì •' ë‘˜ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”
    ì¶”ê°€ ì„¤ëª…ì—†ì´ ê¸ì • ë˜ëŠ” ë¶€ì •ê³¼ ê´€ë ¨ëœ ë‹¨ì–´ 3ê°œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”
    
    #ì œì•½ì¡°ê±´
    ê±´ì„¤ ê´€ë ¨ ê¸°ì‚¬ë§Œ ì¶œë ¥í•œë‹¤
    ê±´ì„¤ ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ê±´ì„¤ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤ ë¼ê³ ë§Œ ëŒ€ë‹µí•œë‹¤
    ê±´ì„¤ ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ëŠ” êµ­ê°€ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤
    ìš”ì ì„ ëª…í™•íˆ í•œë‹¤
    ë¬¸ì¥ì€ ê°„ê²°í•˜ê²Œ ì•Œê¸° ì‰½ê²Œ ì“´ë‹¤
    í•œêµ­ì–´ë¡œ ì¶œë ¥í•œë‹¤

    '''
    gpt_feeds = gpt_feed + gpt_feeds
    prompt = gpt_feeds

if prompt:
    if prompt1_button or prompt2_button :
        # ê¸°ì¡´ ìœ ì € ë©”ì‹œì§€ëŠ” ì‚­ì œí•˜ê¸°
        st.session_state.nmessages = [
            message for message in st.session_state.nmessages
            if message["role"] != "user" or message["hide"] != True
        ]
        st.session_state.nmessages.append({"role": "user", "content": prompt, "hide": True})

    else:
        st.session_state.nmessages.append({"role": "user", "content": prompt, "hide": False})
        with st.chat_message("user"):
            st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        for response in openai.chat.completions.create(
            model=st.session_state["openai_model"],
            # model = 'gpt-3.5-turbo-1106',
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.nmessages
            ],
            stream=True,
        ):
            for chunk in response.choices:
                if chunk.finish_reason == 'stop':
                    break
                full_response += chunk.delta.content
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)
    st.session_state.nmessages.append({"role": "assistant", "content": full_response, "hide": False})

tokenizer = tiktoken.get_encoding("cl100k_base")
tokenizer = tiktoken.encoding_for_model("gpt-4-1106-preview") # gpt-3.5-turbo

with st.expander('í”„ë¡¬í”„íŠ¸ ë³´ê¸°', expanded=False):
    st.write(st.session_state)

message_cnt = 0
token_size = 0
for message in st.session_state['nmessages']:
    if message['role'] == 'system':
        continue
    encoding_result = tokenizer.encode(message['content'])
    token_size = token_size + len(encoding_result)
    message_cnt = message_cnt + 1

if token_size != 0:
    st.caption(f"{message_cnt} ê°œì˜ ëŒ€í™”ì—ì„œ {token_size} í† í°ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ì˜ˆìƒë¹„ìš©ì€ {round(token_size * 0.00001, 2)}$ ì…ë‹ˆë‹¤")
